{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19fa6849",
   "metadata": {},
   "source": [
    "## Team 2022-180's research project front-end with back-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb694e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:20] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:21] \"GET /static/customStyle.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:21] \"GET /static/carousel_05.jpg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:21] \"GET /static/carousel_03.jpg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:21] \"GET /static/carousel_02.jpg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:21] \"GET /static/carousel_04.jpg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:21] \"GET /static/carousel_06.jpg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:21] \"GET /static/img_01.jpg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:21] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:24] \"GET /add-latest-comment HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:24] \"GET /static/bac_04.jpg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:24] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:24] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:24] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:33] \"POST /write HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:33] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:37] \"GET /TransalteToTanglish HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:37] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "[2023-10-30 08:16:40,145] ERROR in app: Exception on /AddWithDataset [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"/var/folders/xs/kp_qnwm552gg46d2vk9sstn40000gn/T/ipykernel_9089/867318169.py\", line 864, in add_with_dataset\n",
      "    data_set = pd.read_excel(r'C:\\Users\\Sajee\\Desktop\\Research-2023 group\\2023-137_Project_PP2\\process_01 output - Tanglish_Comments.xlsx') # change your pc path..\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 457, in read_excel\n",
      "    io = ExcelFile(io, storage_options=storage_options, engine=engine)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 1376, in __init__\n",
      "    ext = inspect_excel_format(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 1250, in inspect_excel_format\n",
      "    with get_handle(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 798, in get_handle\n",
      "    handle = open(handle, ioargs.mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Sajee\\\\Desktop\\\\Research-2023 group\\\\2023-137_Project_PP2\\\\process_01 output - Tanglish_Comments.xlsx'\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:40] \"GET /AddWithDataset HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:42] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "[2023-10-30 08:16:43,593] ERROR in app: Exception on /AddWithDataset [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"/var/folders/xs/kp_qnwm552gg46d2vk9sstn40000gn/T/ipykernel_9089/867318169.py\", line 864, in add_with_dataset\n",
      "    data_set = pd.read_excel(r'C:\\Users\\Sajee\\Desktop\\Research-2023 group\\2023-137_Project_PP2\\process_01 output - Tanglish_Comments.xlsx') # change your pc path..\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 457, in read_excel\n",
      "    io = ExcelFile(io, storage_options=storage_options, engine=engine)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 1376, in __init__\n",
      "    ext = inspect_excel_format(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 1250, in inspect_excel_format\n",
      "    with get_handle(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 798, in get_handle\n",
      "    handle = open(handle, ioargs.mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Sajee\\\\Desktop\\\\Research-2023 group\\\\2023-137_Project_PP2\\\\process_01 output - Tanglish_Comments.xlsx'\n",
      "127.0.0.1 - - [30/Oct/2023 08:16:43] \"GET /AddWithDataset HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:03] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:03] \"GET /static/bac_02.jpg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:13] \"GET /add-latest-comment HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:13] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:13] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:13] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:16] \"GET /genre-analysis HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:17] \"GET /static/bac_03.jpg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:19] \"GET /sentimental-analysis HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:20] \"GET /tanglish-translation HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:20] \"GET /static/bac_01.jpg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:17:21] \"GET /home HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:18:46] \"GET /tanglish-translation HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:18:59] \"POST /uploader HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:19:02] \"GET /cleaningComments HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:19:21] \"GET /predictLanguage HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:19:31] \"GET /basicNLTK HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:19:55] \"GET /translateToTamil HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:00] \"GET /tanglish HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:06] \"GET /save HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:11] \"GET /sentimental-analysis HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:18] \"POST /uploadTanglish HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:21] \"GET /tokenization HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:25] \"GET /count HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:29] \"GET /percentage HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:32] \"GET /decision HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:39] \"GET /genre-analysis HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:46] \"POST /upload HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:49] \"GET /tok HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:51] \"GET /genres HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:53] \"GET /genre_percentage HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:20:55] \"GET /final_decision HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:00] \"GET /add-latest-comment HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [30/Oct/2023 08:21:00] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:00] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:00] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:09] \"POST /write HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:09] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:12] \"GET /TransalteToTanglish HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:13] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "[2023-10-30 08:21:15,596] ERROR in app: Exception on /AddWithDataset [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"/var/folders/xs/kp_qnwm552gg46d2vk9sstn40000gn/T/ipykernel_9089/867318169.py\", line 864, in add_with_dataset\n",
      "    data_set = pd.read_excel(r'C:\\Users\\Sajee\\Desktop\\Research-2023 group\\2023-137_Project_PP2\\process_01 output - Tanglish_Comments.xlsx') # change your pc path..\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 457, in read_excel\n",
      "    io = ExcelFile(io, storage_options=storage_options, engine=engine)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 1376, in __init__\n",
      "    ext = inspect_excel_format(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 1250, in inspect_excel_format\n",
      "    with get_handle(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 798, in get_handle\n",
      "    handle = open(handle, ioargs.mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Sajee\\\\Desktop\\\\Research-2023 group\\\\2023-137_Project_PP2\\\\process_01 output - Tanglish_Comments.xlsx'\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:15] \"GET /AddWithDataset HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:17] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:17] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:17] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "[2023-10-30 08:21:28,146] ERROR in app: Exception on /AddWithDataset [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"/var/folders/xs/kp_qnwm552gg46d2vk9sstn40000gn/T/ipykernel_9089/867318169.py\", line 864, in add_with_dataset\n",
      "    data_set = pd.read_excel(r'C:\\Users\\Sajee\\Desktop\\Research-2023 group\\2023-137_Project_PP2\\process_01 output - Tanglish_Comments.xlsx') # change your pc path..\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 457, in read_excel\n",
      "    io = ExcelFile(io, storage_options=storage_options, engine=engine)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 1376, in __init__\n",
      "    ext = inspect_excel_format(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 1250, in inspect_excel_format\n",
      "    with get_handle(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 798, in get_handle\n",
      "    handle = open(handle, ioargs.mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Sajee\\\\Desktop\\\\Research-2023 group\\\\2023-137_Project_PP2\\\\process_01 output - Tanglish_Comments.xlsx'\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:28] \"GET /AddWithDataset HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:21:31] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:35:16] \"GET /TransalteToTanglish HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:35:17] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:35:17] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:35:17] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n",
      "[2023-10-30 08:35:22,605] ERROR in app: Exception on /AddWithDataset [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/flask/app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"/var/folders/xs/kp_qnwm552gg46d2vk9sstn40000gn/T/ipykernel_9089/867318169.py\", line 864, in add_with_dataset\n",
      "    data_set = pd.read_excel(r'C:\\Users\\Sajee\\Desktop\\Research-2023 group\\2023-137_Project_PP2\\process_01 output - Tanglish_Comments.xlsx') # change your pc path..\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 457, in read_excel\n",
      "    io = ExcelFile(io, storage_options=storage_options, engine=engine)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 1376, in __init__\n",
      "    ext = inspect_excel_format(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\", line 1250, in inspect_excel_format\n",
      "    with get_handle(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 798, in get_handle\n",
      "    handle = open(handle, ioargs.mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Sajee\\\\Desktop\\\\Research-2023 group\\\\2023-137_Project_PP2\\\\process_01 output - Tanglish_Comments.xlsx'\n",
      "127.0.0.1 - - [30/Oct/2023 08:35:22] \"GET /AddWithDataset HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [30/Oct/2023 08:35:24] \"GET /static/Jailer.mp4 HTTP/1.1\" 206 -\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from googletrans import LANGUAGES\n",
    "translator = Translator()\n",
    "\n",
    "from flask import Flask, render_template, redirect, url_for, request \n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "@app.route(\"/home\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "  \n",
    "@app.route(\"/tanglish-translation\")\n",
    "def process_01():\n",
    "    return render_template(\"process_01.html\")\n",
    "\n",
    "@app.route(\"/sentimental-analysis\")\n",
    "def process_02():\n",
    "    return render_template(\"process_02.html\")\n",
    "\n",
    "@app.route(\"/genre-analysis\")\n",
    "def process_03():\n",
    "    return render_template(\"process_03.html\")\n",
    "\n",
    "@app.route(\"/add-latest-comment\")\n",
    "def process_04():\n",
    "    return render_template(\"process_04.html\")\n",
    "\n",
    "@app.route(\"/about\")\n",
    "def about():\n",
    "    return render_template(\"about.html\")\n",
    "\n",
    "#---------------------component-01-------------------------------------------------------------(start)\n",
    "translated_words = []\n",
    "src_language = []\n",
    "dest_language = []\n",
    "translated_tamil_words = []\n",
    "get_Letters = []\n",
    "encode = []\n",
    "encode_list = []\n",
    "\n",
    "# dictionaries for encoding & encoding\n",
    "d_tamil = {\"‡ÆÖ\":\"0\", \"‡ÆÜ\":\"1\", \"‡Æá\":\"2\", \"‡Æà\":\"3\", \"‡Æâ\":\"4\", \"‡Æä\":\"5\", \"‡Æé\":\"6\", \"‡Æè\":\"7\", \"‡Æê\":\"8\", \"‡Æí\":\"9\", \"‡Æì\":\"!\", \"‡Æî\":\"@\",\n",
    "                    \"‡Æï\":\"#\", \"‡Æô\":\"$\", \"‡Æö\":\"%\", \"‡Æú\":\"^\", \"‡Æû\":\"&\", \"‡Æü\":\"*\", \"‡Æ£\":\"-\", \"‡Æ§\":\"+\", \"‡Æ®\":\"=\", \"‡Æ©\":\"a\", \"‡Æ™\":\"b\", \"‡ÆÆ\":\"c\",\"‡ÆØ\":\"d\", \"‡Æ∞\":\"e\", \"‡Æ±\":\"f\", \"‡Æ≤\":\"g\", \"‡Æ≥\":\"h\", \"‡Æ¥\":\"i\", \"‡Æµ\":\"j\", \"‡Æ∑\":\"k\", \"‡Æ∏\":\"l\", \"‡Æπ\":\"m\",\n",
    "                    \"‡Øç\":\"n\", \"‡Ææ\":\"o\", \"‡Æø\":\"p\", \"‡ØÄ\":\"q\", \"‡ØÅ\":\"r\", \"‡ØÇ\":\"s\", \"‡ØÜ\":\"t\", \"‡Øá\":\"u\", \"‡Øà\":\"v\", \"‡Øä\":\"w\", \"‡Øã\":\"x\", \"‡Øå\":\"y\", \"‡Øó\":\"~\", \"‡ÆÉ\":\"z\"\n",
    "                   }\n",
    "\n",
    "d_english = {\"0\":\"A\", \"1\":\"A\", \"2\":\"I\", \"3\":\"I\", \"4\":\"U\", \"5\":\"U\", \"6\":\"E\", \"7\":\"E\", \"8\":\"AI\", \"9\":\"O\", \"!\":\"O\", \"@\":\"AU\",\n",
    "                        \"#\":\"K\", \"$\":\"N\", \"%\":\"S\", \"^\":\"J\", \"&\":\"N\", \"*\":\"D\", \"-\":\"N\", \"+\":\"TH\", \"=\":\"N\", \"a\":\"N\", \"b\":\"P\", \"c\":\"M\", \"d\":\"Y\", \"e\":\"r\", \"f\":\"r\", \"g\":\"L\", \"h\":\"L\", \"i\":\"L\", \"j\":\"V\", \"k\":\"S\", \"l\":\"S\", \"m\":\"H\",\n",
    "                        \"n\":\"\", \"o\":\"a\", \"p\":\"i\", \"q\":\"i\", \"r\":\"u\", \"s\":\"u\", \"t\":\"e\", \"u\":\"e\", \"v\":\"ai\", \"w\":\"o\", \"x\":\"o\", \"y\":\"au\", \"~\":\"\", \"z\":\"H\"        \n",
    "                    }\n",
    "\n",
    "\n",
    "punctuation = '''!()@#&:;ü§úü§õüéÇüòéüíö...‚ù§/%?\\{}$,-üòÇüôè*üëáüî•üò¢üëπüíòüòäüëçüëéüëäüò£ü§ïüòàüòÑ‚ò†üò≠1234567890üòÄüíóü•∞üòçüíØü§©‚ô•Ô∏èü§òüëà‚úåÔ∏èüôÇüí™‚úäüèøüòòü•≥ü§üüò´üòäüíñÔ∏èüåπüëåü•≥ü§óü§Æü§¢üé¨ü•≥ü§Øüòáü§£üèΩüòÉüí•ü§î'''\n",
    "def remove_punctuation(cmt):\n",
    "    cmt_nonpunct = \"\".join([ char for char in cmt if char not in punctuation])                               \n",
    "    return cmt_nonpunct\n",
    "\n",
    "def tokenize(cmt):\n",
    "    tokens = re.split('\\W+', cmt)\n",
    "    return tokens\n",
    "\n",
    "remove_words = []\n",
    "remove_words.append('httpswwwyoutubecomwatchv')\n",
    "remove_words.append('yjlRWSiampt')\n",
    "remove_words.append('href')\n",
    "remove_words.append('br')\n",
    "remove_words.append('Bar')\n",
    "remove_words.append('Kahatiti')\n",
    "remove_words.append('ms')\n",
    "remove_words.append('a')\n",
    "remove_words.append('the')\n",
    "remove_words.append('tha')\n",
    "\n",
    "def remove_stop_words(cmt_tokenize):\n",
    "    clean_cmt = [word for word in cmt_tokenize if word not in remove_words]\n",
    "    return clean_cmt\n",
    "\n",
    "def merge_tokens(cmt):\n",
    "    merge_token = \" \".join(cmt)                                                                                                          \n",
    "    return merge_token\n",
    "\n",
    "\n",
    "# import data-set\n",
    "@app.route('/uploader', methods=['GET', 'POST'])\n",
    "def file_import():\n",
    "    if request.method == 'POST':\n",
    "        global get_file\n",
    "        get_file = request.form['upload-excel']\n",
    "        if get_file != '':\n",
    "            movie_comments = pd.read_excel(get_file)\n",
    "            return render_template(\"process_01.html\", data=movie_comments.to_html(index=False).replace('<th>', '<th style=\"text-align:center; color:#a8031f; font-size:20px \">'))\n",
    "            \n",
    "        return render_template(\"process_01.html\", error=\"Please Import the File!\") # else...\n",
    "    \n",
    "\n",
    "# remove punctuation\n",
    "@app.route(\"/cleaningComments\")\n",
    "def cleaning_process():\n",
    "#     print(get_file)\n",
    "    if get_file != '':\n",
    "        data1 = pd.read_excel(get_file)\n",
    "        data1['Cleaned_Comments'] = data1['Original_Comments'].apply(lambda x: remove_punctuation(x)) # remove punctuation marks\n",
    "        global Cleaned_Comments\n",
    "        Cleaned_Comments = data1['Cleaned_Comments'] \n",
    "        return render_template(\"process_01.html\", cleanedComments=data1.to_html(index=False).replace('<th>', '<th style=\"text-align:center; color:#a8031f; font-size:20px \">'))\n",
    "    return render_template(\"process_01.html\", error1=\"There are no dataset files!\")\n",
    "\n",
    "\n",
    "# predict language & translate to English language\n",
    "@app.route(\"/predictLanguage\")\n",
    "def predict_language():\n",
    "    if get_file != '':\n",
    "        data2 = pd.read_excel(get_file)\n",
    "        for element in Cleaned_Comments:\n",
    "            translated_words.append(translator.translate(element).text) # translate to English language\n",
    "            src_language.append(LANGUAGES[ translator.translate(element).src] ) # predict source language\n",
    "            dest_language.append(LANGUAGES[ translator.translate(element).dest] ) # predict destination language\n",
    "        data2['Cleaned_Comments'] = Cleaned_Comments # add with data2\n",
    "        data2['Source_Language'] = src_language\n",
    "        data2['destination_Language'] = dest_language\n",
    "        data2['English_Comments'] = translated_words\n",
    "        \n",
    "        # store as global variable\n",
    "        global english_comments\n",
    "        english_comments = data2['English_Comments']\n",
    "        global Source_Language\n",
    "        Source_Language = data2['Source_Language']\n",
    "        global destination_Language\n",
    "        destination_Language = data2['destination_Language']\n",
    "        \n",
    "        return render_template(\"process_01.html\", translation=data2.to_html(index=False).replace('<th>', '<th style=\"text-align:center; color:#a8031f; font-size:20px \">'))\n",
    "    \n",
    "    return render_template(\"process_01.html\", error2=\"There are no dataset files!\")\n",
    "\n",
    "\n",
    "# basic NLTK process\n",
    "@app.route(\"/basicNLTK\")\n",
    "def basic_Process():\n",
    "    if get_file != '':\n",
    "        data3 = pd.read_excel(get_file)\n",
    "        \n",
    "        data3['Cleaned_Comments'] = Cleaned_Comments # add with data3\n",
    "        data3['Source_Language'] = Source_Language # add with data3\n",
    "        data3['destination_Language'] = destination_Language # add with data3        \n",
    "        data3['English_Comments'] = english_comments # add with data3\n",
    "        \n",
    "        data3['tokens'] = english_comments.apply(lambda x: tokenize(x)) # tokenization\n",
    "        data3['cleaned_tokens'] = data3['tokens'].apply(lambda x: remove_stop_words(x)) # remove unwanted token\n",
    "        data3['merge_tokens'] = data3['cleaned_tokens'].apply(lambda x: merge_tokens(x)) # merge token\n",
    "        \n",
    "        global tokens, cleaned_tokens, merge_tokens\n",
    "        tokens = data3['tokens']\n",
    "        cleaned_tokens = data3['cleaned_tokens']\n",
    "        merge_tokens = data3['merge_tokens'] \n",
    "        \n",
    "        return render_template(\"process_01.html\", basicNLTK=data3.to_html(index=False).replace('<th>', '<th style=\"text-align:center; color:#a8031f; font-size:20px \">'))\n",
    "    \n",
    "    return render_template(\"process_01.html\", error3=\"There are no dataset files!\")\n",
    "\n",
    "\n",
    "# translate to Tamil language\n",
    "@app.route(\"/translateToTamil\")\n",
    "def translate_tamil():\n",
    "    if get_file != '':\n",
    "        data4 = pd.read_excel(get_file)\n",
    "        for element in merge_tokens:\n",
    "            translated_tamil_words.append(translator.translate(element, dest=\"ta\").text) # translate to Tamil language\n",
    "            \n",
    "        data4['Cleaned_Comments'] = Cleaned_Comments # add with data4\n",
    "        data4['Source_Language'] = Source_Language # add with data4\n",
    "        data4['destination_Language'] = destination_Language # add with data4        \n",
    "        data4['English_Comments'] = english_comments # add with data4\n",
    "        data4['tokens'] = tokens # add with data4\n",
    "        data4['cleaned_tokens'] = cleaned_tokens # add with data4\n",
    "        data4['merge_tokens'] = merge_tokens # add with data4\n",
    "        data4['Tamil_Comments'] = translated_tamil_words # result of translate to tamil language\n",
    "        \n",
    "        global tamil_comments\n",
    "        tamil_comments = data4['Tamil_Comments']\n",
    "        \n",
    "        return render_template(\"process_01.html\", traslateToTamil=data4.to_html(index=False).replace('<th>', '<th style=\"text-align:center; color:#a8031f; font-size:20px \">'))\n",
    "    \n",
    "    return render_template(\"process_01.html\", error4=\"There are no dataset files!\")\n",
    "\n",
    "\n",
    "# encoding & decoding process & get Tanglish as output \n",
    "@app.route(\"/tanglish\")\n",
    "def get_tanglish():\n",
    "    if get_file != '':\n",
    "        data5 = pd.read_excel(get_file)\n",
    "            \n",
    "        data5['Cleaned_Comments'] = Cleaned_Comments \n",
    "        data5['Source_Language'] = Source_Language \n",
    "        data5['destination_Language'] = destination_Language     \n",
    "        data5['English_Comments'] = english_comments \n",
    "        data5['tokens'] = tokens\n",
    "        data5['cleaned_tokens'] = cleaned_tokens\n",
    "        data5['merge_tokens'] = merge_tokens \n",
    "        data5['Tamil_Comments'] = tamil_comments \n",
    "        \n",
    "        # encoding & decoding process\n",
    "        for tamil_cmt in tamil_comments:\n",
    "        # print(tamil_cmt)\n",
    "            for char in tamil_cmt:\n",
    "                get_Letters.append(char) \n",
    "            get_Letters.append(\"\\n \")\n",
    "        TL = ''.join(get_Letters)\n",
    "        encode = re.sub('({})'.format( '|'.join(map(re.escape, dict_tamil.keys() ))), lambda m: dict_tamil[m.group()], TL)\n",
    "        # print(encode)\n",
    "        for c in encode:\n",
    "            encode_list.append(c)\n",
    "        # print(encode_list)\n",
    "\n",
    "        get_result = \"\"\n",
    "        for index, val in enumerate(encode): #----Validation for tamil-letters----\n",
    "            if val=='#' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='$' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='%' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='^' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='&' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='*' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='-' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='+' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='=' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='a' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='b' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='c' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='d' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='e' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='f' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='g' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='h' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='i' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='j' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='k' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='l' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='m' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "        # print(get_result)\n",
    "        encode = ''.join(get_result)\n",
    "        # print(\"========= En-Code =========\", \"\\n\", \"\\n\", encode) \n",
    "\n",
    "        step_2 = re.sub('({})'.format('|'.join(map(re.escape, dict_english.keys() ))), lambda m: dict_english[m.group()], encode)\n",
    "        decode = step_2.capitalize()\n",
    "        # print(\"========= De-Code(Tamil-English Code-mix Data) =========\", \"\\n\", \"\\n\", decode) \n",
    "\n",
    "        store1 = []\n",
    "        store2 = []\n",
    "        store1.append( encode.title().rstrip().split('\\n'))\n",
    "        store2.append( decode.title().rstrip().split('\\n'))\n",
    "        # print(store1)\n",
    "        # print(store2)\n",
    "\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        for front_index in store1:\n",
    "            for second_index in front_index:\n",
    "                temp1.append(second_index)   \n",
    "        #     print(temp1)\n",
    "\n",
    "        for front_index in store2:\n",
    "            for second_index in front_index:\n",
    "                temp2.append(second_index)   \n",
    "        #     print(temp2)\n",
    "\n",
    "        # store into the data-set\n",
    "        data5['encode'] = temp1 \n",
    "        data5['decode(Tanglish_Comment)'] = temp2 \n",
    "        \n",
    "        global encodes, decodes\n",
    "        encodes = data5['encode']\n",
    "        decodes = data5['decode(Tanglish_Comment)']\n",
    "        \n",
    "        return render_template(\"process_01.html\", tanglishComments=data5.to_html(index=False).replace('<th>', '<th style=\"text-align:center; color:#a8031f; font-size:20px \">'))\n",
    "    \n",
    "    return render_template(\"process_01.html\", error5=\"There are no dataset files!\")\n",
    "\n",
    "# Save Tanglish-comments file as an Excel.\n",
    "@app.route(\"/save\") \n",
    "def save_file():\n",
    "    if get_file != '':\n",
    "        data6 = pd.read_excel(get_file)\n",
    "            \n",
    "        data6['Cleaned_Comments'] = Cleaned_Comments # add with data6\n",
    "        data6['Source_Language'] = Source_Language # add with data6\n",
    "        data6['destination_Language'] = destination_Language # add with data6       \n",
    "        data6['English_Comments'] = english_comments # add with data6\n",
    "        data6['tokens'] = tokens # add with data6\n",
    "        data6['cleaned_tokens'] = cleaned_tokens # add with data6\n",
    "        data6['merge_tokens'] = merge_tokens # add with data6\n",
    "        data6['Tamil_Comments'] = tamil_comments # add with data6\n",
    "        data6['encode'] = encodes # add with data6\n",
    "        data6['decode(Tanglish_Comment)'] = decodes # add with data6\n",
    "        \n",
    "        df = pd.DataFrame(data6)\n",
    "\n",
    "        writer = pd.ExcelWriter('process_01 output - Tanglish_Comments.xlsx', engine='xlsxwriter')\n",
    "        df.to_excel(writer, sheet_name=\"tanglish_translator\")\n",
    "        writer.save()\n",
    "        \n",
    "        return render_template(\"process_01.html\", msg_01=\"File Successfully Saved.\")\n",
    "    \n",
    "    return render_template(\"process_01.html\", msg_02=\"Error!!!\")\n",
    "#---------------------component-01---------------------------------------------------------------(end)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------component-02-----------------------------------------------------------(start)\n",
    "positive_list = ['nanraka', 'alakana', 'siranthathu', 'verupadda', 'virumpukiren', 'olukkamana', 'nalla', 'sirantha', 'suppar', 'maas', 'rasikan', 'geya', 'veramathi', 'nalla', 'senjudar', 'nallam', 'paarkalaam', 'senjudar', 'senjudanga', 'neruppu', 'mersal', 'next level', 'level', 'vera kaddam', 'verelevel']\n",
    "negative_list = ['virayam', 'kothikalan', 'keli', 'kalivu', 'kanravi', 'kevalam', 'asinkam', 'kevalamana', 'asinkamana', 'bore', 'boomer', 'beep', 'nallave illa', 'nalla illa', 'poor', 'mayiru', 'bad', 'failure', 'fail', 'sleep']\n",
    "\n",
    "def token(tok):\n",
    "    token = re.split('\\W+', tok)\n",
    "    return token\n",
    "\n",
    "@app.route('/uploadTanglish', methods=['GET', 'POST'])\n",
    "def tanglish_import():\n",
    "    if request.method == 'POST':\n",
    "        global file1\n",
    "        file1 = request.form['upload-tanglish']\n",
    "        if file1 != '':\n",
    "            tanglish_comments = pd.read_excel(file1)\n",
    "            # use this line, if you want to show specific column....\n",
    "            # data_table = pd.concat([ tanglish_comments[tanglish_comments.columns[11:]] ], axis=1)\n",
    "            data_table = tanglish_comments\n",
    "            return render_template(\"process_02.html\",tanglish_dataset=data_table.to_html(index=False).replace('<th>', '<th style=\"text-align:center; color:#a8031f; font-size:20px \">'))\n",
    "            \n",
    "        return render_template(\"process_02.html\", error=\"Please Import the File!\")\n",
    "\n",
    "@app.route(\"/tokenization\")\n",
    "def tokenization():\n",
    "    if file1 != '':\n",
    "        tang_data = pd.read_excel(file1)\n",
    "        \n",
    "        tang_data['Tokens'] = tang_data['decode(Tanglish_Comment)'].apply(lambda x: tokz(x)) # tokenization\n",
    "        global token\n",
    "        token = tang_data['Tokens']\n",
    "#         data_table = pd.concat([ tang_data[tang_data.columns[11:]] ], axis=1)\n",
    "        data_table = tang_data\n",
    "        \n",
    "        return render_template(\"process_02.html\", tokens=data_table.to_html(index=False).replace('<th>', '<th style=\"text-align:center; color:#a8031f; font-size:20px \">'))\n",
    "    \n",
    "    return render_template(\"process_02.html\", err1=\"There are no dataset files!\")\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/count\")\n",
    "def get_count():\n",
    "    global possitive, negative;\n",
    "    \n",
    "    if file1 != '':\n",
    "        tang_data = pd.read_excel(file1)\n",
    "        tang_data['Tokens'] = token #add tokens with tang_data1 files\n",
    "        \n",
    "        positive_comments_count = 0;\n",
    "        negative_comments_count = 0;\n",
    "        \n",
    "        for cmt in tang_data['Tokens']:  \n",
    "            try:  \n",
    "                for word in cmt:\n",
    "                    if word.lower() in positive_list:\n",
    "                        while word.lower() not in positive_list:\n",
    "                            continue\n",
    "                        positive_comments_count += 1 \n",
    "#                         print(word)\n",
    "                        break\n",
    "\n",
    "                for word in cmt:\n",
    "                    if word.lower() in negative_list:\n",
    "                        while word.lower() not in negative_list:\n",
    "                            continue\n",
    "                        negative_comments_count += 1\n",
    "#                         print(word)\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        status = 'Positive Comments: ', positive_comments_count,  ' Negative Comments:', negative_comments_count\n",
    "        pos = positive_comments_count;\n",
    "        neg = negative_comments_count;\n",
    "        \n",
    "        positive_rating = pos/(pos + neg)\n",
    "        positve_percentage = round(positive_rating*100, 2)\n",
    "\n",
    "        negative_rating = neg/(pos + neg)\n",
    "        negative_percentage = round(negative_rating*100, 2)\n",
    "        \n",
    "        possitive = positve_percentage;\n",
    "        negative = negative_percentage;\n",
    "        \n",
    "        return render_template(\"process_02.html\", counts=status)\n",
    "    return render_template(\"process_02.html\", err2=\"Counting failed!\")\n",
    "    \n",
    "\n",
    "@app.route(\"/percentage\")\n",
    "def count_percentage():\n",
    "    global pos_rating, neg_rating;\n",
    "    \n",
    "    if possitive != 0.0 and negative != 0.0:    \n",
    "        pos_rating = possitive\n",
    "        neg_rating = negative\n",
    "        \n",
    "        status = 'Positive percentage: ', pos_rating , '%', 'Negative percentage:', neg_rating , '%'\n",
    "        \n",
    "        return render_template(\"process_02.html\", percentage = status)\n",
    "    return render_template(\"process_02.html\", err3=\"There are no comments!\")\n",
    "\n",
    "@app.route(\"/decision\")\n",
    "def final_decision():\n",
    "    if pos_rating != 0.0 and neg_rating != 0.0:    \n",
    "        \n",
    "        if pos_rating > neg_rating:\n",
    "            status = 'This movie has more \"positive\" review than negative review. So, It is a Good Movie!'\n",
    "        else:\n",
    "            status = 'This movie has more \"negative\" review than positive review. So, It is a Average Movie!'\n",
    "        \n",
    "        return render_template(\"process_02.html\", decision = status)\n",
    "    return render_template(\"process_02.html\", err4=\"There are no comments percentage!\")\n",
    "#---------------------component-02--------------------------------------------------------------(end)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------component-03-----------------------------------------------------------------(start)\n",
    "comedy_list = ['sirippu', 'siriththan', 'comedy']\n",
    "action_list = ['sandai','sandaik', 'sandaikal', 'sandaikasikal', 'thi', 'Nerupu', 'Kothikalan', 'Athiradi', 'kathaikkalam']\n",
    "love_list = ['kathal', 'lovers', 'kathalarkal', 'kathalar', 'kaviyam', 'virumpukiren', 'olukkamana', 'alakana']\n",
    "horror_list = ['payama', 'payanthudan', 'thikil', 'pei']\n",
    "\n",
    "def tokz(tok):\n",
    "    token = re.split('\\W+', tok)\n",
    "    return token\n",
    "\n",
    "@app.route('/upload', methods=['GET', 'POST'])\n",
    "def upload():\n",
    "    if request.method == 'POST':\n",
    "        global file2\n",
    "        file2 = request.form['upload-tanglish']\n",
    "        if file2 != '':\n",
    "            uploaded_dataset = pd.read_excel(file2)\n",
    "#             data_table = pd.concat([ uploaded_dataset[uploaded_dataset.columns[11:]] ], axis=1)\n",
    "            data_table = uploaded_dataset\n",
    "            return render_template(\"process_03.html\", dataset=data_table.to_html(index=False).replace('<th>', '<th style=\"text-align:center; color:#a8031f; font-size:20px \">'))\n",
    "            \n",
    "        return render_template(\"process_03.html\", error=\"Please Import the File!\")\n",
    "    \n",
    "\n",
    "@app.route(\"/tok\")\n",
    "def make_tokens():\n",
    "    if file2 != '':\n",
    "        tanglish = pd.read_excel(file2)\n",
    "        \n",
    "        tanglish['Tokens'] = tanglish['decode(Tanglish_Comment)'].apply(lambda x: tokz(x)) # tokenization\n",
    "        global tok\n",
    "        tok = tanglish['Tokens']\n",
    "#         data_table = pd.concat([ tanglish[tanglish.columns[11:]] ], axis=1)\n",
    "        data_table = tanglish\n",
    "        return render_template(\"process_03.html\", tokens=data_table.to_html(index=False).replace('<th>', '<th style=\"text-align:center; color:#a8031f; font-size:20px \">'))\n",
    "    \n",
    "    return render_template(\"process_03.html\", err1=\"There are no dataset files!\")\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/genres\")\n",
    "def get_genres():\n",
    "    global comedy, love, horror, action;\n",
    "    \n",
    "    if file2 != '':\n",
    "        tanglish = pd.read_excel(file2)\n",
    "        tanglish['Tokens'] = tok \n",
    "        \n",
    "        comedy_comments_count = 0;\n",
    "        action_comments_count = 0;\n",
    "        love_comments_count = 0;\n",
    "        horror_comments_count = 0;\n",
    "\n",
    "        for cmt in tanglish['Tokens']:  \n",
    "            try:  \n",
    "                for word in cmt:\n",
    "                    if word.lower() in comedy_list:\n",
    "                        while word.lower() not in comedy_list:\n",
    "                            continue\n",
    "                        comedy_comments_count += 1 \n",
    "                        # print(word)\n",
    "                        break\n",
    "\n",
    "                for word in cmt:\n",
    "                    if word.lower() in action_list:\n",
    "                        while word.lower() not in action_list:\n",
    "                            continue\n",
    "                        action_comments_count += 1\n",
    "                        # print(word)\n",
    "                        break\n",
    "\n",
    "                for word in cmt:\n",
    "                    if word.lower() in love_list:\n",
    "                        while word.lower() not in love_list:\n",
    "                            continue\n",
    "                        love_comments_count += 1\n",
    "                        # print(word)\n",
    "                        break\n",
    "\n",
    "                for word in cmt:\n",
    "                    if word.lower() in horror_list:\n",
    "                        while word.lower() not in horror_list:\n",
    "                            continue\n",
    "                        horror_comments_count += 1\n",
    "                        # print(word)\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        status = 'Comedy Comments: ', comedy_comments_count, 'Action Comments: ', action_comments_count, 'Love Comments: ', love_comments_count, 'Horror Comments: ', horror_comments_count;\n",
    "        c = comedy_comments_count;\n",
    "        a = action_comments_count;\n",
    "        l = love_comments_count;\n",
    "        h = horror_comments_count;\n",
    "\n",
    "        comedy_rating = c/(c+a+l+h)\n",
    "        comedy_percentage = round(comedy_rating*100, 2)\n",
    "\n",
    "        action_rating = a/(c+a+l+h)\n",
    "        action_percentage = round(action_rating*100, 2)\n",
    "        \n",
    "        love_rating = l/(c+a+l+h)\n",
    "        love_percentage = round(love_rating*100, 2)\n",
    "        \n",
    "        horror_rating = h/(c+a+l+h)\n",
    "        horror_percentage = round(horror_rating*100, 2)\n",
    "\n",
    "        comedy = comedy_percentage;\n",
    "        action = action_percentage;\n",
    "        love = love_percentage;\n",
    "        horror = horror_percentage;\n",
    "\n",
    "        return render_template(\"process_03.html\", genres=status)\n",
    "    return render_template(\"process_03.html\", err2=\"Counting failed!\")\n",
    "\n",
    "@app.route(\"/genre_percentage\")\n",
    "def genre_percent():      \n",
    "    status = 'Comedy percentage: ', comedy ,'%','action percentage:', action , '%','love percentage:', love , '%','horror percentage:', horror , '%'\n",
    "    return render_template(\"process_03.html\", percentage = status)\n",
    "\n",
    "\n",
    "@app.route(\"/final_decision\")\n",
    "def make_decision(): \n",
    "    resu = 'No decision...'\n",
    "    \n",
    "# ----Testing purpose----   \n",
    "#     comedy = 40\n",
    "#     action = 40\n",
    "#     love = 10\n",
    "#     horror = 5\n",
    "    \n",
    "    if comedy>action and comedy>love and comedy>horror:\n",
    "        resu = 'It is a Comedy genre movie!'\n",
    "    elif action>comedy and action>love and action>horror:\n",
    "        resu = 'It is a Action genre movie!'\n",
    "    elif love>comedy and love>action and love>horror:\n",
    "        resu = 'It is a Love genre movie!'\n",
    "    elif horror>comedy and horror>action and horror>love:\n",
    "        resu = 'It is a Horror genre movie!'  \n",
    "    elif (comedy==action) and (comedy>love and comedy>horror):\n",
    "        resu = 'It is a Comedy & Action genres movie!' \n",
    "    elif (comedy==love) and (comedy>action and comedy>horror):\n",
    "        resu = 'It is a Comedy & Love genres movie!' \n",
    "    elif (comedy==horror) and (comedy>love and comedy>action):\n",
    "        resu = 'It is a Comedy & Horror genres movie!'\n",
    "    elif (action==love) and (action>comedy and action>horror):\n",
    "        resu = 'It is a Action & Love genres movie!'\n",
    "    elif (action==horror) and (action>comedy and action>love):\n",
    "        resu = 'It is a Action & Horror genres movie!'\n",
    "    elif (love==horror) and (love>comedy and love>action):\n",
    "        resu = 'It is a Love & Horror genres movie!'\n",
    "    return render_template(\"process_03.html\", deci = resu)\n",
    "\n",
    "#---------------------component-03-------------------------------------------------------------------(end)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------component-04-------------------------------------------------------------------(start)\n",
    "\n",
    "punctuation = '''!()@#&:;\"ü§úü§õüéÇüòéüíö...‚ù§/%?\\{}$,-üòÇüôè*üëáüî•üò¢üëπüíòüòäüëçüëéüëäüò£ü§ïüòàüòÑ‚ò†üò≠1234567890üòÄüíóü•∞üòçüíØü§©‚ô•Ô∏èü§òüëà‚úåÔ∏èüôÇüí™‚úäüèøüòòü•≥ü§üüò´üòäüíñÔ∏èüåπüëåü•≥ü§óü§Æü§¢üé¨ü•≥ü§Øüòáü§£üèΩüòÉüí•ü§î'''\n",
    "\n",
    "# Tamil-English dictionary\n",
    "dict_tamil = {\"‡ÆÖ\":\"0\", \"‡ÆÜ\":\"1\", \"‡Æá\":\"2\", \"‡Æà\":\"3\", \"‡Æâ\":\"4\", \"‡Æä\":\"5\", \"‡Æé\":\"6\", \"‡Æè\":\"7\", \"‡Æê\":\"8\", \"‡Æí\":\"9\", \"‡Æì\":\"!\", \"‡Æî\":\"@\",\n",
    "                    \"‡Æï\":\"#\", \"‡Æô\":\"$\", \"‡Æö\":\"%\", \"‡Æú\":\"^\", \"‡Æû\":\"&\", \"‡Æü\":\"*\", \"‡Æ£\":\"-\", \"‡Æ§\":\"+\", \"‡Æ®\":\"=\", \"‡Æ©\":\"a\", \"‡Æ™\":\"b\", \"‡ÆÆ\":\"c\",\"‡ÆØ\":\"d\", \"‡Æ∞\":\"e\", \"‡Æ±\":\"f\", \"‡Æ≤\":\"g\", \"‡Æ≥\":\"h\", \"‡Æ¥\":\"i\", \"‡Æµ\":\"j\", \"‡Æ∑\":\"k\", \"‡Æ∏\":\"l\", \"‡Æπ\":\"m\",\n",
    "                    \"‡Øç\":\"n\", \"‡Ææ\":\"o\", \"‡Æø\":\"p\", \"‡ØÄ\":\"q\", \"‡ØÅ\":\"r\", \"‡ØÇ\":\"s\", \"‡ØÜ\":\"t\", \"‡Øá\":\"u\", \"‡Øà\":\"v\", \"‡Øä\":\"w\", \"‡Øã\":\"x\", \"‡Øå\":\"y\", \"‡Øó\":\"~\", \"‡ÆÉ\":\"z\"\n",
    "                   }\n",
    "\n",
    "dict_english = {\"0\":\"A\", \"1\":\"A\", \"2\":\"I\", \"3\":\"I\", \"4\":\"U\", \"5\":\"U\", \"6\":\"E\", \"7\":\"E\", \"8\":\"AI\", \"9\":\"O\", \"!\":\"O\", \"@\":\"AU\",\n",
    "                        \"#\":\"K\", \"$\":\"N\", \"%\":\"S\", \"^\":\"J\", \"&\":\"N\", \"*\":\"D\", \"-\":\"N\", \"+\":\"TH\", \"=\":\"N\", \"a\":\"N\", \"b\":\"P\", \"c\":\"M\", \"d\":\"Y\", \"e\":\"r\", \"f\":\"r\", \"g\":\"L\", \"h\":\"L\", \"i\":\"L\", \"j\":\"V\", \"k\":\"S\", \"l\":\"S\", \"m\":\"H\",\n",
    "                        \"n\":\"\", \"o\":\"a\", \"p\":\"i\", \"q\":\"i\", \"r\":\"u\", \"s\":\"u\", \"t\":\"e\", \"u\":\"e\", \"v\":\"ai\", \"w\":\"o\", \"x\":\"o\", \"y\":\"au\", \"~\":\"\", \"z\":\"H\"        \n",
    "                    }\n",
    "\n",
    "\n",
    "@app.route('/write', methods=['GET', 'POST'])\n",
    "def submit_cmt():\n",
    "    if request.method == 'POST':\n",
    "        global user_cmts\n",
    "        user_cmts = request.form['user_cmt']\n",
    "        if user_cmts != '':\n",
    "            return render_template(\"process_04.html\", wrote_1=user_cmts);\n",
    "        \n",
    "        return render_template(\"process_04.html\", error1=\"Please Type Your Comments!\");\n",
    "    \n",
    "\n",
    "@app.route(\"/TransalteToTanglish\")\n",
    "def trans_to_tang():\n",
    "    if user_cmts != '':\n",
    "        global english_cmt, tamil_cmt, decode, encode;\n",
    "        \n",
    "        # remove punctuation only\n",
    "        pure_cmt = [''.join(c for c in user_cmts if c not in punctuation)];\n",
    "        \n",
    "        # translate to english\n",
    "        for a in pure_cmt:\n",
    "            english_cmt = translator.translate(a, dest=\"en\").text;\n",
    "\n",
    "        # translate to tamil   \n",
    "        tamil_cmt = translator.translate(english_cmt, dest=\"tamil\").text;\n",
    "        \n",
    "        # translate to tanglish\n",
    "        # convert to tamil-english code-mix data, apply algorithm for en-code and de-code\n",
    "        get_Letters = []\n",
    "        encode = []\n",
    "        encode_list = []\n",
    "\n",
    "        for tc in tamil_cmt:\n",
    "        #     print(tamil_cmt)\n",
    "            get_Letters.append(tc) \n",
    "        TL = ''.join(get_Letters)\n",
    "        encode = re.sub('({})'.format( '|'.join(map(re.escape, dict_tamil.keys() ))), lambda m: dict_tamil[m.group()], TL)\n",
    "        # print(encode)\n",
    "        for c in encode:\n",
    "            encode_list.append(c)\n",
    "        # print(encode_list)\n",
    "\n",
    "        get_result = \"\"\n",
    "\n",
    "        for index, val in enumerate(encode):\n",
    "            if val=='#' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='$' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='%' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='^' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='&' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='*' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='-' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='+' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='=' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='a' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='b' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='c' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='d' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='e' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='f' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='g' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='h' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='i' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='j' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='k' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='l' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "            elif val=='m' and encode[index+1]!='n' and encode[index+1]!='o' and encode[index+1]!='p' and encode[index+1]!='q' and encode[index+1]!='r' and encode[index+1]!='s' and encode[index+1]!='t' and encode[index+1]!='u' and encode[index+1]!='v' and encode[index+1]!='w' and encode[index+1]!='x' and encode[index+1]!='y':\n",
    "                set_value = (val+dict_tamil[\"‡Ææ\"])\n",
    "                encode_list[index] = set_value\n",
    "                get_result = encode_list\n",
    "\n",
    "        # print(get_result)\n",
    "        encode = ''.join(get_result)\n",
    "\n",
    "        step_2 = re.sub('({})'.format('|'.join(map(re.escape, dict_english.keys() ))), lambda m: dict_english[m.group()], encode)\n",
    "        decode = step_2.capitalize()\n",
    "\n",
    "        status = 'Code-Mixed data- ', encode , ' Tanglish data- ', decode\n",
    "        \n",
    "        return render_template(\"process_04.html\", wrote_2=status);\n",
    "    return render_template(\"process_04.html\", error2=\"There are no comment available...\")\n",
    "\n",
    "\n",
    "@app.route(\"/AddWithDataset\")\n",
    "def add_with_dataset():\n",
    "    if decode != '':\n",
    "        \n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        data_set = pd.read_excel(r'C:\\Users\\Sajee\\Desktop\\Research-2023 group\\2023-137_Project_PP2\\process_01 output - Tanglish_Comments.xlsx') # change your pc path..\n",
    "        data_table = pd.concat([ data_set[data_set.columns[11:]] ], axis=1) # get only tanglish column(11)... \n",
    "#         print(data_table)\n",
    "\n",
    "\n",
    "        # data spilt and store into the list\n",
    "        store = []\n",
    "        store.append( decode.title().rstrip().split('\\n'))\n",
    "\n",
    "        temp = []\n",
    "        for front_index in store:\n",
    "            for second_index in front_index:\n",
    "                temp.append(second_index)   \n",
    "#         print(temp)\n",
    "\n",
    "\n",
    "        # create new column for latest tanglish data\n",
    "        new_column = pd.DataFrame({ \"latest_tanglish\": temp})\n",
    "\n",
    "        # merge 2 columns data\n",
    "        latest_data_set = data_table[\"decode(Tanglish_Comment)\"].append(new_column[\"latest_tanglish\"]).reset_index(drop=True)\n",
    "\n",
    "        #  store dataset\n",
    "        store_data = pd.DataFrame(latest_data_set)\n",
    "        \n",
    "#         print(data_table) # before add latest Tanglish cmt....\n",
    "#         print(\"-----------------------------------------------------\")\n",
    "#         print(store_data) # after added latest Tanglish cmt....\n",
    "\n",
    "        writer = pd.ExcelWriter('result-latest_tanglish_dataset.xlsx', engine='xlsxwriter')\n",
    "        store_data.to_excel(writer, sheet_name=\"tanglish_latest_list\")\n",
    "\n",
    "        # rename column name \n",
    "        renamed_dataset = store_data.rename(mapper={ 0: 'decode(Tanglish_Comment)' }, axis='columns') \n",
    "        renamed_dataset.to_excel(writer, sheet_name=\"tanglish_latest_list\")\n",
    "\n",
    "        # store the latest dataset\n",
    "        response = writer.save() \n",
    "        print(\"latest data saved :\" , response)\n",
    "        \n",
    "        return render_template(\"process_04.html\", wrote_03=\"File saved successfully!\")\n",
    "    return render_template(\"process_04.html\", error3=\"Error!!!\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#---------------------component-04-------------------------------------------------------------------(end)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run()\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32894c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
